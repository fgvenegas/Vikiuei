{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VAL = 'CLEVR_val'\n",
    "QUESTIONS_VAL_PATH = '/home/chrisams/Datasets/CLEVR_sample/questions/CLEVR_val_questions_sample.json'\n",
    "FEATS_VAL_DIR = '/home/chrisams/Datasets/CLEVR_sample/regions-MiniCLEVR/regions-miniCLEVR-val'\n",
    "QUESTIONS_EMBEDDING_VAL = '/home/chrisams/Datasets/CLEVR_sample/questions_val_glove_embeddings.npy'\n",
    "MAPPER_VAL_PATH = 'q2img_val_mapper.json'\n",
    "\n",
    "DATASET_TRAIN = 'CLEVR_train'\n",
    "QUESTIONS_TRAIN_PATH = '/home/chrisams/Datasets/CLEVR_sample/questions/CLEVR_train_questions_sample.json'\n",
    "FEATS_TRAIN_DIR = '/home/chrisams/Datasets/CLEVR_sample/regions-MiniCLEVR/regions-miniCLEVR-train'\n",
    "QUESTIONS_EMBEDDING_TRAIN = '/home/chrisams/Datasets/CLEVR_sample/questions_train_glove_embeddings.npy'\n",
    "MAPPER_TRAIN_PATH = 'q2img_train_mapper.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q2img_mapper(questions_path, output_filename):\n",
    "    '''Saves a json file with the mapper of question to img.\n",
    "    This will be used by the dataloader.\n",
    "    '''\n",
    "    with open(questions_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(data['image_index'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_q2img_mapper(QUESTIONS_VAL_PATH, MAPPER_VAL_PATH)\n",
    "get_q2img_mapper(QUESTIONS_TRAIN_PATH, MAPPER_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# with open('q2img_val_mapper.json', 'r') as f:\n",
    "#     data1 = json.load(f)\n",
    "# with open(QUESTIONS_VAL_PATH, 'r') as f:\n",
    "#     data2 = json.load(f)['image_index']\n",
    "\n",
    "# assert data1 == data2\n",
    "\n",
    "# with open('q2img_train_mapper.json', 'r') as f:\n",
    "#     data1 = json.load(f)\n",
    "# with open(QUESTIONS_TRAIN_PATH, 'r') as f:\n",
    "#     data2 = json.load(f)['image_index']\n",
    "\n",
    "# assert data1 == data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottomFeaturesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, feats_dir, questions_path, mapper_path, dataset):\n",
    "        self.q2img_mapper = self.load_mapper(mapper_path)\n",
    "        self.q_emb = np.load(questions_path)\n",
    "        self.feats_dir = feats_dir\n",
    "        if dataset in ['CLEVR_val', 'CLEVR_train']:\n",
    "            self.get_img_name = lambda x: f'{dataset}_{str(x).zfill(6)}.npy'\n",
    "        \n",
    "    def load_mapper(self, mapper_path):\n",
    "        with open(mapper_path, 'r') as f:\n",
    "            mapper = json.load(f)\n",
    "        # Sort keys by number.\n",
    "        q_idxs = sorted(map(int, mapper.keys()))\n",
    "        new_mapper = [None] * len(q_idxs)\n",
    "        for i, q_idx in enumerate(q_idxs):\n",
    "            new_mapper[i] = mapper[str(q_idx)]\n",
    "        return new_mapper\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.q2img_mapper)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.q_emb[idx]\n",
    "        img_filename = self.get_img_name(self.q2img_mapper[idx])\n",
    "        img = np.load(os.path.join(self.feats_dir, img_filename), allow_pickle=True).item()['features']\n",
    "        return {'image': torch.from_numpy(img),\n",
    "                'question': torch.from_numpy(question)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_feats_dataset = BottomFeaturesDataset(FEATS_VAL_DIR, QUESTIONS_EMBEDDING_VAL, MAPPER_VAL_PATH, DATASET_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(bottom_feats_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 36, 2048]) torch.Size([4, 25, 300])\n",
      "1 torch.Size([4, 36, 2048]) torch.Size([4, 25, 300])\n",
      "2 torch.Size([4, 36, 2048]) torch.Size([4, 25, 300])\n",
      "3 torch.Size([4, 36, 2048]) torch.Size([4, 25, 300])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['question'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
